'use client'

import React, { useState, useRef, useEffect } from 'react'

interface ChatMessage {
  id: number
  type: 'user' | 'ai'
  text: string
  audioSize?: number
  timestamp: Date
}

export default function VoiceChat() {
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [chatHistory, setChatHistory] = useState<ChatMessage[]>([])
  const [error, setError] = useState('')
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioRef = useRef<HTMLAudioElement>(null)
  const chatContainerRef = useRef<HTMLDivElement>(null)

  // æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ æ™‚ã«è‡ªå‹•ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight
    }
  }, [chatHistory])

  const addMessage = (type: 'user' | 'ai', text: string, audioSize?: number) => {
    const newMessage: ChatMessage = {
      id: Date.now(),
      type,
      text,
      audioSize,
      timestamp: new Date()
    }
    setChatHistory(prev => [...prev, newMessage])
  }

  const startRecording = async () => {
    try {
      console.log('ğŸ¤ éŒ²éŸ³é–‹å§‹...')
      setError('')
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      const options = { mimeType: 'audio/webm' }
      if (!MediaRecorder.isTypeSupported(options.mimeType)) {
        console.log('webméå¯¾å¿œã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå½¢å¼ä½¿ç”¨')
        mediaRecorderRef.current = new MediaRecorder(stream)
      } else {
        mediaRecorderRef.current = new MediaRecorder(stream, options)
      }

      audioChunksRef.current = []

      mediaRecorderRef.current.ondataavailable = (event) => {
        console.log('ğŸµ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å—ä¿¡:', event.data.size, 'bytes')
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = async () => {
        console.log('ğŸ›‘ éŒ²éŸ³åœæ­¢ã€å‡¦ç†é–‹å§‹...')
        await processRecording()
      }

      mediaRecorderRef.current.start()
      setIsRecording(true)
      console.log('âœ… éŒ²éŸ³é–‹å§‹æˆåŠŸ')

    } catch (error) {
      console.error('âŒ éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', error)
      setError(`éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: ${error}`)
    }
  }

  const stopRecording = () => {
    console.log('â¹ï¸ éŒ²éŸ³åœæ­¢ä¸­...')
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
      setIsProcessing(true)
    }
  }

  const processRecording = async () => {
    try {
      console.log('ğŸ”„ éŸ³å£°å‡¦ç†é–‹å§‹...')
      
      if (audioChunksRef.current.length === 0) {
        throw new Error('éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“')
      }

      // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
      const recordedAudioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
      console.log('ğŸ“Š éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º:', recordedAudioBlob.size, 'bytes')

      if (recordedAudioBlob.size === 0) {
        throw new Error('éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™')
      }

      // FormDataä½œæˆ
      const formData = new FormData()
      formData.append('audio', recordedAudioBlob, 'recording.webm')
      
      console.log('ğŸ“¤ éŸ³å£°èªè­˜APIå‘¼ã³å‡ºã—...')

      // Whisper API ã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
      const transcribeResponse = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData
      })

      if (!transcribeResponse.ok) {
        const errorText = await transcribeResponse.text()
        throw new Error(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${transcribeResponse.status} - ${errorText}`)
      }

      const transcribeData = await transcribeResponse.json()
      const transcribedText = transcribeData.text || ''
      
      console.log('âœ… éŸ³å£°èªè­˜æˆåŠŸ:', transcribedText)

      if (!transcribedText.trim()) {
        throw new Error('éŸ³å£°ãŒèªè­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')
      }

      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
      addMessage('user', transcribedText, recordedAudioBlob.size)

      // Chat APIå‘¼ã³å‡ºã—
      console.log('ğŸ¤– AIå¿œç­”ç”Ÿæˆé–‹å§‹...')
      
      const chatResponse = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message: transcribedText.trim()
        })
      })

      console.log('ğŸ“¡ Chat APIå¿œç­”ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:', chatResponse.status)

      if (!chatResponse.ok) {
        const errorText = await chatResponse.text()
        console.error('âŒ Chat APIã‚¨ãƒ©ãƒ¼è©³ç´°:', errorText)
        throw new Error(`AIå¿œç­”ã‚¨ãƒ©ãƒ¼: ${chatResponse.status}`)
      }

      // AIãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†ä¿®æ­£ç‰ˆï¼‰
      let aiResponseText = 'AIå¿œç­”ã‚’å†ç”Ÿä¸­...'
      try {
        const encodedAiText = chatResponse.headers.get('X-AI-Response-Text')
        if (encodedAiText) {
          aiResponseText = decodeURIComponent(encodedAiText)
          console.log('âœ… AIãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸ:', aiResponseText)
        }
      } catch (decodeError) {
        console.warn('âš ï¸ AIãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼:', decodeError)
        aiResponseText = 'AIå¿œç­”ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ï¼‰'
      }
      
      // AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
      addMessage('ai', aiResponseText)

      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
      const audioArrayBuffer = await chatResponse.arrayBuffer()
      console.log('ğŸµ AIéŸ³å£°å—ä¿¡:', audioArrayBuffer.byteLength, 'bytes')
      
      if (audioArrayBuffer.byteLength === 0) {
        throw new Error('AIéŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™')
      }

      // éŸ³å£°å†ç”Ÿ
      const playbackAudioBlob = new Blob([audioArrayBuffer], { type: 'audio/mpeg' })
      const audioUrl = URL.createObjectURL(playbackAudioBlob)
      
      if (audioRef.current) {
        audioRef.current.src = audioUrl
        await audioRef.current.play()
        console.log('ğŸ”Š AIéŸ³å£°å†ç”Ÿé–‹å§‹')
      }

    } catch (error) {
      console.error('âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°:', error)
      setError(`å‡¦ç†ã‚¨ãƒ©ãƒ¼: ${error}`)
      addMessage('ai', `ã‚¨ãƒ©ãƒ¼: ${error}`)
    } finally {
      setIsProcessing(false)
      audioChunksRef.current = []
    }
  }

  const toggleRecording = () => {
    if (isRecording) {
      stopRecording()
    } else {
      startRecording()
    }
  }

  const clearHistory = () => {
    setChatHistory([])
    setError('')
  }

  return (
    <div className="flex flex-col h-screen max-h-screen bg-gray-50">
      {/* ãƒ˜ãƒƒãƒ€ãƒ¼ */}
      <div className="flex-shrink-0 bg-white shadow-sm border-b p-4">
        <div className="flex justify-between items-center">
          <h1 className="text-2xl font-bold text-gray-800">ğŸ¤ AIéŸ³å£°ãƒãƒ£ãƒƒãƒˆ</h1>
          <button
            onClick={clearHistory}
            className="px-3 py-1 text-sm bg-gray-200 hover:bg-gray-300 rounded-md transition-colors"
          >
            å±¥æ­´ã‚¯ãƒªã‚¢
          </button>
        </div>
      </div>

      {/* ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚¨ãƒªã‚¢ï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ï¼‰ */}
      <div 
        ref={chatContainerRef}
        className="flex-1 overflow-y-auto p-4 space-y-4"
      >
        {chatHistory.length === 0 ? (
          <div className="text-center text-gray-500 mt-8">
            <p>ğŸ¤ ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„</p>
          </div>
        ) : (
          chatHistory.map((message) => (
            <div key={message.id} className={`flex ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}>
              <div className={`max-w-2xl w-fit p-4 rounded-lg ${
                message.type === 'user' 
                  ? 'bg-blue-500 text-white' 
                  : 'bg-white text-gray-800 shadow-sm border'
              }`}>
                <div className="flex items-center gap-2 mb-2">
                  <span className="text-sm font-semibold">
                    {message.type === 'user' ? 'ğŸ‘¤ ã‚ãªãŸ' : 'ğŸ¤– AI'}
                  </span>
                  <span className="text-xs opacity-70">
                    {message.timestamp.toLocaleTimeString()}
                  </span>
                  {message.audioSize && (
                    <span className="text-xs opacity-70">
                      ({(message.audioSize / 1024).toFixed(1)}KB)
                    </span>
                  )}
                </div>
                <p className="text-sm leading-relaxed">{message.text}</p>
              </div>
            </div>
          ))
        )}
      </div>

      {/* éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚¨ãƒªã‚¢ */}
      <div className="flex-shrink-0 bg-white border-t p-6">
        <div className="flex flex-col items-center space-y-4">
          <button
            onClick={toggleRecording}
            disabled={isProcessing}
            className={`w-16 h-16 rounded-full flex items-center justify-center text-2xl font-bold transition-all duration-200 ${
              isRecording 
                ? 'bg-red-500 hover:bg-red-600 text-white animate-pulse' 
                : isProcessing
                ? 'bg-yellow-500 text-white cursor-not-allowed'
                : 'bg-green-500 hover:bg-green-600 text-white shadow-lg hover:shadow-xl'
            }`}
          >
            {isProcessing ? 'â³' : isRecording ? 'â¹ï¸' : 'ğŸ¤'}
          </button>
          
          <p className="text-sm text-gray-600 text-center">
            {isProcessing ? 'å‡¦ç†ä¸­...' : isRecording ? 'éŒ²éŸ³ä¸­ - ã‚¯ãƒªãƒƒã‚¯ã§åœæ­¢' : 'ã‚¯ãƒªãƒƒã‚¯ã§éŒ²éŸ³é–‹å§‹'}
          </p>

          {error && (
            <div className="w-full max-w-2xl p-3 bg-red-50 border border-red-200 rounded-lg">
              <p className="text-red-700 text-sm">âŒ {error}</p>
            </div>
          )}
        </div>
      </div>

      <audio ref={audioRef} className="hidden" />
    </div>
  )
}
