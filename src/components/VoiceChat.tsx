'use client'

import React, { useState, useRef } from 'react'

export default function VoiceChat() {
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [transcription, setTranscription] = useState('')
  const [response, setResponse] = useState('')
  const [audioSize, setAudioSize] = useState(0)
  const [error, setError] = useState('')
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioRef = useRef<HTMLAudioElement>(null)

  const startRecording = async () => {
    try {
      console.log('ğŸ¤ éŒ²éŸ³é–‹å§‹...')
      setError('')
      setTranscription('')
      setResponse('')
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      // MediaRecorderè¨­å®šï¼ˆå®‰å…¨ãªå½¢å¼ï¼‰
      const options = { mimeType: 'audio/webm' }
      if (!MediaRecorder.isTypeSupported(options.mimeType)) {
        console.log('webméå¯¾å¿œã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå½¢å¼ä½¿ç”¨')
        mediaRecorderRef.current = new MediaRecorder(stream)
      } else {
        mediaRecorderRef.current = new MediaRecorder(stream, options)
      }

      audioChunksRef.current = []

      mediaRecorderRef.current.ondataavailable = (event) => {
        console.log('ğŸµ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å—ä¿¡:', event.data.size, 'bytes')
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = async () => {
        console.log('ğŸ›‘ éŒ²éŸ³åœæ­¢ã€å‡¦ç†é–‹å§‹...')
        await processRecording()
      }

      mediaRecorderRef.current.start()
      setIsRecording(true)
      console.log('âœ… éŒ²éŸ³é–‹å§‹æˆåŠŸ')

    } catch (error) {
      console.error('âŒ éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', error)
      setError(`éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: ${error}`)
    }
  }

  const stopRecording = () => {
    console.log('â¹ï¸ éŒ²éŸ³åœæ­¢ä¸­...')
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
      setIsProcessing(true)
    }
  }

  const processRecording = async () => {
    try {
      console.log('ğŸ”„ éŸ³å£°å‡¦ç†é–‹å§‹...')
      
      if (audioChunksRef.current.length === 0) {
        throw new Error('éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“')
      }

      // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆéŒ²éŸ³ç”¨ï¼‰
      const recordedAudioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
      setAudioSize(recordedAudioBlob.size)
      console.log('ğŸ“Š éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º:', recordedAudioBlob.size, 'bytes')

      if (recordedAudioBlob.size === 0) {
        throw new Error('éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™')
      }

      // FormDataä½œæˆï¼ˆå®‰å…¨ãªé€ä¿¡æ–¹æ³•ï¼‰
      const formData = new FormData()
      formData.append('audio', recordedAudioBlob, 'recording.webm')
      
      console.log('ğŸ“¤ éŸ³å£°èªè­˜APIå‘¼ã³å‡ºã—...')

      // Whisper API ã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
      const transcribeResponse = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData
      })

      if (!transcribeResponse.ok) {
        const errorText = await transcribeResponse.text()
        throw new Error(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${transcribeResponse.status} - ${errorText}`)
      }

      const transcribeData = await transcribeResponse.json()
      const transcribedText = transcribeData.text || ''
      
      console.log('âœ… éŸ³å£°èªè­˜æˆåŠŸ:', transcribedText)
      setTranscription(transcribedText)

      if (!transcribedText.trim()) {
        throw new Error('éŸ³å£°ãŒèªè­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')
      }

      // Chat APIå‘¼ã³å‡ºã—ï¼ˆå®‰å…¨ãªJSONé€ä¿¡ï¼‰
      console.log('ğŸ¤– AIå¿œç­”ç”Ÿæˆé–‹å§‹...')
      
      const chatResponse = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message: transcribedText.trim()
        })
      })

      console.log('ğŸ“¡ Chat APIå¿œç­”ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:', chatResponse.status)

      if (!chatResponse.ok) {
        const errorText = await chatResponse.text()
        console.error('âŒ Chat APIã‚¨ãƒ©ãƒ¼è©³ç´°:', errorText)
        throw new Error(`AIå¿œç­”ã‚¨ãƒ©ãƒ¼: ${chatResponse.status}`)
      }

      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
      const audioArrayBuffer = await chatResponse.arrayBuffer()
      console.log('ğŸµ AIéŸ³å£°å—ä¿¡:', audioArrayBuffer.byteLength, 'bytes')
      
      if (audioArrayBuffer.byteLength === 0) {
        throw new Error('AIéŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™')
      }

      // éŸ³å£°å†ç”Ÿï¼ˆå†ç”Ÿç”¨ï¼‰
      const playbackAudioBlob = new Blob([audioArrayBuffer], { type: 'audio/mpeg' })
      const audioUrl = URL.createObjectURL(playbackAudioBlob)
      
      if (audioRef.current) {
        audioRef.current.src = audioUrl
        await audioRef.current.play()
        console.log('ğŸ”Š AIéŸ³å£°å†ç”Ÿé–‹å§‹')
      }

      setResponse('AIéŸ³å£°å¿œç­”ã‚’å†ç”Ÿä¸­...')

    } catch (error) {
      console.error('âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°:', error)
      setError(`å‡¦ç†ã‚¨ãƒ©ãƒ¼: ${error}`)
    } finally {
      setIsProcessing(false)
      audioChunksRef.current = []
    }
  }

  const toggleRecording = () => {
    if (isRecording) {
      stopRecording()
    } else {
      startRecording()
    }
  }

  return (
    <div className="flex flex-col items-center space-y-6 p-8">
      <h1 className="text-3xl font-bold text-gray-800">ğŸ¤ AIéŸ³å£°ãƒãƒ£ãƒƒãƒˆ</h1>
      
      <div className="flex flex-col items-center space-y-4">
        <button
          onClick={toggleRecording}
          disabled={isProcessing}
          className={`w-20 h-20 rounded-full flex items-center justify-center text-2xl font-bold transition-all duration-200 ${
            isRecording 
              ? 'bg-red-500 hover:bg-red-600 text-white animate-pulse' 
              : isProcessing
              ? 'bg-yellow-500 text-white cursor-not-allowed'
              : 'bg-green-500 hover:bg-green-600 text-white'
          }`}
        >
          {isProcessing ? 'â³' : isRecording ? 'â¹ï¸' : 'ğŸ¤'}
        </button>
        
        <p className="text-sm text-gray-600">
          {isProcessing ? 'å‡¦ç†ä¸­...' : isRecording ? 'éŒ²éŸ³ä¸­ - ã‚¯ãƒªãƒƒã‚¯ã§åœæ­¢' : 'ã‚¯ãƒªãƒƒã‚¯ã§éŒ²éŸ³é–‹å§‹'}
        </p>
      </div>

      {audioSize > 0 && (
        <div className="text-sm text-blue-600">
          éŸ³å£°: {(audioSize / 1024).toFixed(1)}KB / æ–‡å­—: {transcription.length}
        </div>
      )}

      {transcription && (
        <div className="max-w-2xl w-full p-4 bg-blue-50 rounded-lg">
          <h3 className="font-semibold text-blue-800 mb-2">ğŸ‘¤ ã‚ãªãŸ:</h3>
          <p className="text-blue-700">{transcription}</p>
        </div>
      )}

      {response && (
        <div className="max-w-2xl w-full p-4 bg-green-50 rounded-lg">
          <h3 className="font-semibold text-green-800 mb-2">ğŸ¤– AI:</h3>
          <p className="text-green-700">{response}</p>
        </div>
      )}

      {error && (
        <div className="max-w-2xl w-full p-4 bg-red-50 rounded-lg">
          <h3 className="font-semibold text-red-800 mb-2">âŒ ã‚¨ãƒ©ãƒ¼:</h3>
          <p className="text-red-700">{error}</p>
        </div>
      )}

      <audio ref={audioRef} className="hidden" />
    </div>
  )
}
